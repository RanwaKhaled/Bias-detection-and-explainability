# Bias-detection-and-explainability
## Challenge description
The goal is to detect and analyze gender bias in job applications; for that we need to classify applicant descriptions into Hired vs Not Hired categories based on certain attributes in the description. 
This is done over the following phases:
- Data generation and preparation
- Classifier Training 
- Bias detection and measurement 
- Explainability of some predictions
- Bias mitigation of biased models
## Dataset:
The provided data is a table containing the following attributes:
### Features:
Age - Gender - Experience Years - Previous Companies Worked - Interview Score - Skill Score - Personality Score - Recruitment Strategy
### Target:
Hiring Decision (Target Variable)
Description: Outcome of the hiring decision.
Categories:
0: Not hired
1: Hired
Data Type: Binary (Integer).
### Goal:
The goal from this tabular data is to generate a text description of each candidate like the following:
 *"I am a Male candidate, 20.0 years old, with a Bachelor's (Type 2) degree and 2.0 years of experience across 5.0 companies. I scored 80.0 in the interview, with a skill score of 90.0 and personality score of 69.0. I applied through Moderate Strategy."*

This description is generated by filling in some place holders with the information from the table in the following template:
*I am a [GENDER] candidate, [AGE] years old, with a [DEGREE] degree and [EXPERIENCE] years of experience across [COMPANIES] companies. I scored [INTERVIEW] in the interview, with a skill score of [SKILL] and personality score of [PERS]. I applied through [STRAT] Strategy.*

The generated descriptions were added to a new column in the dataset called resume_text.

#### Tabular data is uploaded as a csv: `data.csv` and model weights can be found on kaggle through this <a href="">link</a>
